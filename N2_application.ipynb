{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling mit Python II\n",
    "\n",
    "Nach der grundlegenden Verwendung von `Series` und `DataFrame` und den `Jupyter-Notebook` gucken wir uns nun deren Einsatz mit Daten ein. Dabei werden wir wie folgt durchgehen:\n",
    "\n",
    "- Ein möglicher Ablauf einer Datenaufbereitung wird durch das Durchgehen dieses Notebooks ermöglicht und kurz mit Teilüberschriften und kurzen Beschreibungen (am Anfang und Ende) beschrieben \n",
    "- Dabei werden die jeweiligen Schritte durch Aufgaben beschrieben, die in Code-Zellen stehen.\n",
    "- Versuche die Aufgaben zunächste selbst mit Hife der Dokumentation und des letzten Notebooks zu lösen.\n",
    "- Solltest du nicht weiterkommen, kannst du in den nächsten Zellen verschiedene Hilfen( enden mit `help`) durch die `%load` Funktion herbeirufen.\n",
    "- Die Lösung der Aufgaben lassen sich dann in dem Load mit Endung `lsg` finden.\n",
    "\n",
    "Wir werden das Vorgehen ganz kurz durchgehen ;-). Hier also der erste Abschnitt, der zu lösen ist. Ich werde dabei weitere Hinweise geben, um das System zu erklären.\n",
    "Die Aufgabe Bild einfügen ist auch weiter unten zu sehen\n",
    "\n",
    "\n",
    "# Den `%load` Befehl auskommentieren und und die Zelle ausführen, um die Lösung zusehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load N1_T1_import_lsg.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# ausführen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hier kommt dein Code für die Aufgabe N1_T1 hin! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lösung ##\n",
    "# %load N1_T1_import_lsg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataframe from CSV\n",
    "# use utf-8\n",
    "df_penguin = pd.read_csv(\"./penguins_size.csv\", sep=';', decimal=',')\n",
    "# we can read json, excel, sql, and more ... https://pandas.pydata.org/pandas-docs/stable/reference/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Allgemeine Informationen angeben ###\n",
    "# Gib die ersten und letzten 5 Einträge aus, um ein Gefühl für die Daten zu erhalten \n",
    "display(df_penguin)\n",
    "# lasse dir Informationen über die Datentypen, Anzahl an Einträgen und Spalten angeben\n",
    "df_penguin.info()  # https://pbpython.com/pandas_dtypes.html\n",
    "# gib grundlegende Statistiken über alle berechenbare Werte aus\n",
    "# view first lines of dataset\n",
    "#display(df_penguin)\n",
    "#display(df_penguin.tail())\n",
    "#display(df_penguin[-10:])\n",
    "# df.tail() for last lines and df for (reduced) view of the whole Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finden von fehlenden und fehlerhaften Werten ###\n",
    "# Lasse dir alle Einträge ausgeben, die einen NaN (\"Not a Number\") Eintrag enthalten --> gucke wo etwas fehlt\n",
    "for columns in df_penguin.columns:\n",
    "    display(df_penguin.query(f'{columns}.isnull()'))\n",
    "# oder df_penguin[df_penguin.isnull().any(axis=1)]\n",
    "\"\"\"\n",
    "- Einträge 3 und 339 sind für alle Berechnungen unbrauchbar\n",
    "- bei den Einträgen 8,9,10,11,47,246,286,324,336,339 müssten wir nur weglassen, wenn wir uns das Geschlecht angucken\n",
    "\"\"\"\n",
    "### Lass die alle Einträge angeben, die in den nominalen Spalten vorkommen --> suche nach Fehlern ###\n",
    "for columns in df_penguin.columns:\n",
    "    print(df_penguin[columns].unique())\n",
    "    display(df_penguin.groupby(columns).count())\n",
    "\n",
    "\n",
    "### Betrachtung von stetigen Werten ###\n",
    "\n",
    "df_penguin.describe()  # --> gucken, ob sehr hohe/kleine Werte rauskommen oder so.\n",
    "# Lasse dir alle Werte als Grafik anzeigen, die eine metrische Spalte haben --> suche nach Außreißern/Fehlern falsche Skalen\n",
    "#for column in df_penguin.columns:\n",
    "#    s_temp = df_penguin[column]\n",
    "#    if \"float64\" == str(s_temp.dtype):\n",
    "#        display(s_temp.plot.hist())\n",
    "\n",
    "#plt.figure()\n",
    "plt.figure()\n",
    "df_penguin.hist()\n",
    "plt.figure()\n",
    "# df_penguin.plot.hist()\n",
    "df_penguin[\"culmen_length_mm\"].hist()\n",
    "df_penguin[\"culmen_depth_mm\"].hist()\n",
    "df_penguin[\"flipper_length_mm\"].hist()\n",
    "df_penguin[\"body_mass_g\"].hist()\n",
    "\n",
    "plt.figure()\n",
    "df_penguin.boxplot(column=[\"body_mass_g\", \"flipper_length_mm\"])\n",
    "# +++ Flipperlänge in cm umwandeln, damit dort nicht so hohe Werte stehen\n",
    "\n",
    "# umgang mit NaN\n",
    "df_penguin_nona_1 = df_penguin.drop([3,339])  # oder df_penguin.dropna nutzen blöd, wenn es mehrfach lassen wollen besser\n",
    "# Umgang mit fehlerhaften werten\n",
    "df_penguin_nona_1[\"species\"] = df_penguin_nona_1[\"species\"].replace([\"Adellie\", \"Chinnstrap\"], [\"Adelie\", \"Chinstrap\"])\n",
    "df_penguin_nona_1[\"island\"] = df_penguin_nona_1[\"island\"].replace([\"DReam\", \"DrEam\", \"BIscoe\", \"BisCoe\"], [\"Dream\", \"Dream\", \"Biscoe\", \"Biscoe\"])\n",
    "df_penguin_nona_1[\"sex\"] = df_penguin_nona_1[\"sex\"].replace([\"MALe\", \"FEmALE\",\",\"], [\"MALE\",\"FEMALE\",np.NaN])\n",
    "display(df_penguin_nona_1)\n",
    "\n",
    "#df_penguin = df_penguin[df_penguin.isna().any(axis=1)]\n",
    "#display(df_penguin)\n",
    "\n",
    "# !!! vielleicht noch normailisieren? Also eine der Sachen umbauen (in cm statt mm) und dann mit Apply ändern lassen\n",
    "# !!! gucken, ob ein Unterschied bei Männern und Frauen ist\n",
    "#  \n",
    "for columns in df_penguin_nona_1.columns:\n",
    "    print(df_penguin_nona_1[columns].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genauere Untersuchung der Daten ###\n",
    "\n",
    "plt.figure()\n",
    "df_penguin.query('sex == \"MALE\" & species == \"Adelie\"')[\"culmen_length_mm\"].hist()\n",
    "df_penguin_adelie_float = df_penguin.query('sex == \"FEMALE\" and species == \"Adelie\"')[[\"culmen_length_mm\",\"culmen_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]]\n",
    "display(df_penguin_adelie_float)\n",
    "## nach Insel gruppieren und so \n",
    "display(df_penguin.groupby(\"sex\").count())\n",
    "\n",
    "display(df_penguin.query('species == \"Adelie\"').describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look again\n",
    "df_penguin.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen, dass es einige Einträge gibt, die fehlende Werte haben (vorallem beim Sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge Penguin_iters zusammenfügen mit Inseln ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the journal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataframe from CSV\n",
    "df_journal = pd.read_csv(\"./FlourishOA_Data/api_journal11-13-17.csv\",  encoding=\"ISO-8859-1\", sep=',', decimal='.')\n",
    "# we can read json, excel, sql, and more ... https://pandas.pydata.org/pandas-docs/stable/reference/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view first 10 lines of dataset\n",
    "df_journal.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get basic information about datatypes, entries and columns\n",
    "df_journal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there only 0 and 1 in `is_hybrid`?\n",
    "df_journal.groupby(\"is_hybrid\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data-type for is_hybrid\n",
    "df_journal[\"is_hybrid\"] = df_journal[\"is_hybrid\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look again\n",
    "df_journal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the different categories\n",
    "df_journal.groupby(\"category\", sort=True).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all categorys to uppercase\n",
    "category_series = df_journal[\"category\"]\n",
    "df_journal[\"category\"] = df_journal[\"category\"].map(lambda x: x if x is np.nan else x.upper() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the different categories again\n",
    "df_journal.groupby(\"category\", sort=True).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at one specific category \"AGRICULTURE\" \n",
    "category_series_filter = df_journal[df_journal[\"category\"].notnull()][\"category\"].map(lambda x: \"AGRICULTURE\" in x)\n",
    "category_series_filter\n",
    "df_journal[df_journal[\"category\"].notnull()][category_series_filter]\n",
    "df_journal[df_journal[\"category\"].notnull()][category_series_filter].groupby(\"category\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different seperator in mulitcategories. Lets find them\n",
    "category_series_filter = df_journal[df_journal[\"category\"].notnull()][\"category\"].map(lambda x: \".\" in x)\n",
    "df_journal[df_journal[\"category\"].notnull()][category_series_filter]\n",
    "df_journal[df_journal[\"category\"].notnull()][category_series_filter].groupby(\"category\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \".\" with \" |\"\n",
    "df_journal[\"category\"] = df_journal[\"category\"].map(lambda x: x if x is np.nan else x.replace(\".\", \" |\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at Results with specific category \"PHILOSOPHY\"\n",
    "category_series_filter = df_journal[df_journal[\"category\"].notnull()][\"category\"].str.contains(\"PHILOSOPHY\") #df[df[\"category\"].notnull()][\"category\"].str.contains()\n",
    "\n",
    "df_journal[df_journal[\"category\"].notnull()][category_series_filter]\n",
    "df_journal[df_journal[\"category\"].notnull()][category_series_filter].groupby(\"category\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get a list of all Categories\n",
    "\n",
    "# 1. split categories\n",
    "def category_string_to_list(category_string):\n",
    "    category_list = [x.strip() for x in str(category_string).split('|')]\n",
    "    return category_list\n",
    "\n",
    "category_list_series = df_journal[\"category\"].map(category_string_to_list)\n",
    "category_list_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. creating a set of all categorys ant get it length\n",
    "category_set = set()\n",
    "\n",
    "category_lists = category_list_series.tolist()\n",
    "for category_list in category_lists:\n",
    "    category_set.update(category_list)\n",
    "\n",
    "print(category_set)\n",
    "len(category_set)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge journal and price Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Number of ununique columns for possible merge\n",
    "def get_ununique_count(df, column):\n",
    "    return df[column].count() - df[column].nunique()\n",
    "\n",
    "print(df_price.columns)\n",
    "for column in df_price.columns:\n",
    "    print(column, get_ununique_count(df_price, column))\n",
    "\n",
    "print(df_journal.columns)\n",
    "for column in df_journal.columns:\n",
    "    print(column)\n",
    "    print(get_ununique_count(df_journal, column))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging two dataframes\n",
    "\n",
    "df_merge = pd.merge(df_journal, df_price, how='inner', left_on='issn', right_on='journal_id',\n",
    "         left_index=False, right_index=False, sort=True,\n",
    "         suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "         validate=None)[[\"journal_name\", \"price\", \"category\", \"influence_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the merged data\n",
    "print(df_merge.info())\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get generall informations about the price of articles\n",
    "df_merge.price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a lineplot\n",
    "df_merge.price.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogramm with 16 bins\n",
    "df_merge.price.plot.hist(bins=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all articles with price 0\n",
    "df_merge[df_merge.price == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get grouped information about journals\n",
    "table = pd.pivot_table(df_merge, values=[\"price\"], index=\"journal_name\", aggfunc={'price': [min, max ,np.sum, len, np.mean]})\n",
    "table.sort_values(by=(\"price\", \"sum\"), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export table to excel-format\n",
    "table.to_excel(\"././FlourishOA_Data/journal-price_pivot-table.xlsx\")\n",
    "# want more possibilities to work with excel? https://pbpython.com/improve-pandas-excel-output.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Für Notebook 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und zum Schluss noch eini\n",
    "\n",
    "### [Keyboard Shortcuts](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Custom%20Keyboard%20Shortcuts.html)\n",
    "\n",
    "| Command | Action |\n",
    "| --- | --- |\n",
    "|Ctrl + Enter | run selected cells |\n",
    "|Alt + Enter | run the current cell, insert below |\n",
    "|Shift + Enter | run the current cell, select below |\n",
    "|Ctrl + S | save and checkpoint |\n",
    "\n",
    "### [Magic-Commands and othes](https://ipython.readthedocs.io/en/stable/interactive/magics.html)\n",
    "\n",
    "| Command | Action |\n",
    "| --- | --- |\n",
    "| %load python_file.py| load code from python_file.py into Cell |\n",
    "| %run python_file.py| run python_file.py |\n",
    "| %time | times how long a cell needs to finish |\n",
    "| %ls | shows file in the current directory |\n",
    "| %pwd | shows path of the current directory |\n",
    "\n",
    "Want more? Here are [28 Tips, Tricks and Shortcuts](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show visualisation inside the notebook (not needed anymore in newer versions)\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Achtung!!! Die `.plot`-Methode hat einige Eigenheiten, was gemeinsam in einem Bild dargestellt wird. Dabei lässt sich folgendes Merken:\n",
    "\n",
    "- Bei Aufruf einer `plot`-Methode eines `DataFrame` oder bei dem Aufruf der Methode `plt.figure()` wird ein neues Canvas erstellt\n",
    "- Bei Aufruf einer Plot-Methode einer Serie wird dieses auf der letzten Canvas mit dargestellt\n",
    "\n",
    "Hier einige Beispiel, um dies besser zu visualisieren"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df_grades' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9292ea061c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Alle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_grades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"grade\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_grades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_fruit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Numbers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_fruit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Numbers\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Cost\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_grades' is not defined"
     ]
    }
   ],
   "source": [
    "# Alle\n",
    "df_grades[\"grade\"].plot.hist(bins=6, grid=False)\n",
    "df_grades[\"size\"].plot.line()\n",
    "df_fruit[\"Numbers\"].plot.hist()\n",
    "df_fruit.plot.scatter(x=\"Numbers\",y=\"Cost\")\n",
    "df_grades[\"size\"].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wie du siehtst, wurden die drei Aufrufe von `plot` alle zusammen in einem Diagramm dargestellt, was eine nützliche Funktion darstellt. Soll eine Visualisierung in einem neuen Diagramm dargestellt werden, muss vorher die Methode `plt.figure()` ausegführt werden, um ein neue Grundlage zu erstellen, auf der die Diagramme geschrieben werden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_list_with_new_value(value, replace_list, new_value)\n",
    "#      if value in replace_list:\n",
    "#           return new_value\n",
    "#       else:\n",
    "#            return value\n",
    "\n",
    "df_penguin_nona_1[\"species\"] = df_penguin_nona_1[\"species\"].apply(\n",
    "    replace_list_with_new_value,\n",
    "    args = []\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}